# 9.28
## 任务
　学习项目中的语义分割预测等部分
## 问题
　项目代码方法可以很容易查询了解，但是要理解需要将方法串连起来，形成一个模块，理解整个模块的功能  
 
　模块的功能是与深度学习有关，但是深度学习的知识储备不足
## 措施
　在看各个模块时同时了解深度学习中的内容，加深理解，如果理解不足的或者多个模块结合询问师兄
# 10.27
 ## 任务
　重新训练语义分割的结果，并将其转化为对应的rgb图片，并将其用于未来语义预测模型(使用rgb图片代替原始的预测训练集)。

 ## 结果
　成功将原始训练集转化为对应的rgb图片，并且用于训练，训练的精度为0.44左右
  <br><br>
  ![转化结果](/images/seg_rgb.png)
# 11.2
 ## 任务
　提升语义预测模型的精度

 ## 结果
　精度由0.44提升到0.5左右，但是仍然不高，需要再寻求其他方法
  <br><br>
  ![转化结果](/images/evalute3.png)
  
# 11.9
## 任务
　去查看测试的数据集，观察数据帧是不是连续的，会不会有跳帧的情况。同时了解这个预测模型的精度上限是什么。
　继续提升预测模型的精度，我的想法是将其中的光流网络替换成精度更高的网络
## 结果
　数据帧基本连续，不会有跳帧的情况，精度上限未知，但是师兄说最好在0.55-0.6,越高越好
　将flownetc替换为flownets和PWCNet精度均没有得到提升
　![转化结果](/images/evalute4.png)
　![转化结果](/images/PWCNet.png)
# 11.16
## 任务
　去测试每一个模块的精度是什么，查看整体模型精度是卡在哪个模块上
## 结果
　由于数据集是cityscapes，只有语义的标签，如果想测试每一个模块的标签，需要光流等标签，但是cityscapes不提供，因此需要去做调研，在没有标签的情况下如何去测试单个模块精度
## 调研
* **生成标签去测试模型精度**

  
　**1. 采用人工标注**

 
　　现实中基本不可行，因为光流的标签标注是像素级别的，比语义分割的标注更加麻烦，它需要计算每一个像素前后帧之间的位移方向和位移大小，而不是像语义分割将某一个区域划分为类别即可。cityscapes每一张图片大小是1024*2048(大约有20万个像素)，那么对这20万个像素需要标注位移方向和大小耗费时间是巨大的。

  
　**2. 采用类似kitti 2012/2015数据集的方法标注**

 
　　KITTI数据集的标注信息是基于激光坐标系的3D标注信息，通过3d点云投影到下一帧的图像中获得像素的位移信息，最终得到光学流场。但是cityscape是图像数据，并没有图像间的点云信息，无法通过点云计算像素位移。

  
* **无标签测试模型精度**

  
　**1. 考虑无监督的方法**(咨询陈敏捷师兄)

 
　　无监督的方法主要应用于在没有数据标签的时候如何去训练模型的参数，因此它不能去正确的评估模型的精度，同时采用无监督评估模型精度也要关注任务本身，聚类就可以无需标注，但是我们的模块是光流的估计，测试精度仍然需要标签

  
　**2. 查找无标签评估模型模型**


| 论文   | 来源  | 具体细节    |
| - | - | - |
| Label-Free Model Evaluation with Semi-Structured Dataset Representations  | CVPR | 提出了一种新的结构化数据表示方法，用来改进autoeval任务，以强化回归学习 |
| Are Labels Always Necessary for Classifier Accuracy Evaluation?  | CVPR | 考虑训练集和测试集之间的分布差异及其如何影响分类器准确性来研究AutoEval。测量数据集之间的分布差异信息，再用模型去学习差异分布之间的变换与精度变换之间的关系，然后再通过测试集与训练集的分布变换预测模型在测试集的精度 |
| K-means Clustering Based Feature Consistency Alignment for Label-free Model Evaluation  | CVPR |基于K-means聚类的特征一致性对齐表示各种数据集中的分布变化 ，构建一个动态回归模型，使该模型学习到分布与精度的关系，再用其去预测测试集的精度 |


# 11.23
## 任务
把握任务的核心，提升模型的精度，做一个语义分割预测模型的调研，了解其它的语义分割预测模型，查找是否有更好的语义分割预测模型应有到我们的框架中去
## 调研结果
* S2S方法

| 论文 | 来源 | 模型思路 | 精度 |
| - | - | - | - |
|Predicting Deeper into the Future of Semantic Segmentation|ICCV|采用X2X的模型思路，但是输入的是语义分割，然后预测语义分割，X2X模型思路是输入图片，预测图片，再进行语义分割，同时还引入了生成对抗网络| 59.4 |
|Future semantic segmentation with convolutional lstm|cvpr|用resnet101网络结构提取四个连续语义分割帧的四个维度的特征，再分别对四个维度的特征进行convLSTM以预测未来的四个维度的特征，最后再使用1*1卷积，上采样使这几个维度的特征聚合在一起，最终生成未来的语义分割帧|60.1|


* M2M方法(我们当前使用的模型)

| 论文 | 来源 | 模型思路 | 精度 |
| - | - | - | - |
|Recurrent Flow-Guided Semantic Forecasting|cvpr|使用flownet网络生成光流特征，再对光流特征进行cnovLSTM生成未来的光流特征，将未来的光流特征通过warp层映射到当前的语义分割帧上，生成未来的语义分割帧|67.1|


* F2F方法

| 论文 | 来源 | 模型思路 | 精度 |
| - | - | - | - |
|Segmenting the Future|RAL|从RGB图片到语义分割帧，分为学生网络和教师网络。学生网络执行预测任务，教师网络用于使用未来的rgb帧对学生网络进行指导(通过损失函数)。学生网络分为三个部分，编码器(VGG19),预测模型(3d卷积网络),解码器(生成语义分割)。教师网络，生成蒸馏损失用于训练学生网络|57.1|
|Predictive Feature Learning for Future Segmentation Prediction|ICCV|在语义分割的特征提取和预测模块之间，加入一个自动编码器，学习预测特征而不是分割特征，再将学习到的预测特征引入预测模块，生成未来帧的预测特征，然后通过解码器生成分割特征|71.1|

* F2MF方法


| 论文 | 来源 | 模型思路 | 精度 |
| - | - | - | - |
|Warp to the Future: Joint Forecasting of Features and Feature Motion|cvpr|引入了一个F2M模块，用于接收输入的特征并生成光流场，并将输入的特征warp到未来的特征，同时F2F模块也用于生成未来的特征，再将F2M模块和F2F模块生成的未来特征进行加权和，最终经过上采样卷积生成未来的语义分割|69.6|


# 11.30
## 任务
将论文提供的代码的caffe结构改为pytorch结构，与师兄改的pytorch结构进行对比，查看是否是结构错误导致精度不足，并与师兄讨论代码的实现细节
## 结果
我改的pytorch结构与师兄的基本一致，不同之处在于师兄的结构将前面的光流特征提取的那些层替换成为了FlownetC层，因为这两个是一致的，而且改为FlownetC的层也可以使用别人预训练好的权重参数，总体来说不是模型结构错误导致的精度不足

# 12.7
## 任务
继续想办法提高模型精度
## 结果
参考了flownetC中超参数的设定，对学习率，权重衰减等参数进行了调整，模型精度达到了53.4
之后又借鉴了其它的调参方法，对预训练好的权重的网络进行一个较低学习率的调整，对于未训练的进行较高的学习率调整，同时对优化器(optimizer)进行一个调整，将SGD改为了Adam，因为Adam会考虑过去的梯度对现在的影响，使得模型不易陷入到局部解中，最终模型的精度提高到了60.4
* 参数设置

  ```
  final_model_wt4(0.5336):
	configurations.py:self.momentum = 0.95,self.weight_decay = 4e-4
	optimizer:{'params': flownet_weight_params, 'lr':0.00005, 'weight_decay': 2e-4, 'momentum':0.95},
 		{'params': flownet_bias_params, 'lr':0.00005, 'weight_decay': 0, 'momentum':0.95}
  ```
　![转化结果](/images/model_final_wt4.png)
 * 参数设置

  ```
  model_final_wt2_Adam(0.604):#优化器为Adam
	configurations.py:self.momentum = 0.95,self.weight_decay = 3e-4
	optimizer:{'params': flownet_weight_params, 'lr':0.0001, 'weight_decay': 3e-4,},
 		{'params': flownet_bias_params, 'lr':0.0001, 'weight_decay': 0}
  ```
　![转化结果](/images/model_final_wt2_Adam.png)
